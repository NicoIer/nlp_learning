{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.functional as f\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "from torch import Tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 通用编码器架构"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \"\"\"编码器 解码器架构的基本编码器接口\"\"\"\n",
    "    def __init__(self,**kwargs):\n",
    "        super(Encoder, self).__init__()\n",
    "    def forward(self,x,*args):\n",
    "        raise NotImplementedError"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 通用解码器架构"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    \"\"\"编码器 解码器架构的基本解码器接口\"\"\"\n",
    "    def __init__(self,**kwargs):\n",
    "        super(Decoder, self).__init__()\n",
    "    def forward(self,x,state):\n",
    "        raise NotImplementedError\n",
    "    def init_state(self,enc_outputs,*args):\n",
    "        raise NotImplementedError"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 通用Seq2Seq架构"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "class EncoderDecoder(nn.Module):\n",
    "    def __init__(self,encoder:Encoder,decoder:Decoder,**kwargs):\n",
    "        super(EncoderDecoder, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "    def forward(self,enc_x,dec_x,*args):\n",
    "        enc_outputs = self.encoder(enc_x,*args)\n",
    "        dec_state = self.decoder.init_state(enc_outputs,*args)\n",
    "        return self.decoder(dec_x,dec_state)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 实现一个Seq2SeqEncoder"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "class Seq2SeqEncoder(Encoder):\n",
    "    def __init__(self,vocab_size,embedding_dim,num_hidden,num_layers,dropout=0,**kwargs):\n",
    "        super(Seq2SeqEncoder, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size,embedding_dim)\n",
    "        self.rnn = nn.GRU(embedding_dim,num_hidden,num_layers,dropout=dropout,batch_first=True)\n",
    "    def forward(self,x,*args):\n",
    "        \"\"\"\n",
    "        :param x:  形状[batch_size,seq_len]\n",
    "        :param args:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        # 进行词嵌入 embedded形状 [batch_size,seq_len,embedding_dim]\n",
    "        embedded = self.embedding(x)\n",
    "        # 获取最后输出 与 隐层状态\n",
    "        # output形状与embedded相同(需要设置batch_first=True)\n",
    "        # state形状 [num_layers,batch_size,embedding_dim]\n",
    "        output,state = self.rnn(embedded)\n",
    "        return output,state"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "试一试\n",
    "构造一个[batch_size=4,seq_len=7]的批量数据\n",
    "output形状:[batch_size,seq_len,num_hidden]\n",
    "state形状 :[num_layer,batch_size,num_hidden]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "outputs": [
    {
     "data": {
      "text/plain": "(torch.Size([4, 7, 256]), torch.Size([6, 4, 256]))"
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder = Seq2SeqEncoder(vocab_size=10,embedding_dim=256,num_hidden=256,num_layers=6)\n",
    "encoder.eval()\n",
    "x = torch.zeros((4,7),dtype=torch.long)\n",
    "output,state = encoder(x)\n",
    "output.shape,state.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 实现一个Seq2SeqDecoder\n",
    "需要注意的地方:\n",
    "1.从编码器ht中获取额外的输入序列信息:只选了最后一个layer进行获取\n",
    "2.torch下的RNN模型输出的state(ht)的形状为 [num_layer,batch_size,num_hidden]\n",
    "3.给定的输入序列的形状为 [batch_size,seq_len] 在与2拼接的时候需要注意转置"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "outputs": [],
   "source": [
    "class Seq2SeqDecoder(Decoder):\n",
    "    def __init__(self,vocab_size,embedding_dim,num_hidden,num_layers,dropout=0,**kwargs):\n",
    "        super(Seq2SeqDecoder, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size,embedding_dim)\n",
    "        self.rnn = nn.GRU(embedding_dim+num_hidden,num_hidden,num_layers,dropout=dropout,batch_first=True)\n",
    "        self.classify = nn.Linear(num_hidden,vocab_size)\n",
    "    def forward(self,x,state):\n",
    "        \"\"\"\n",
    "        :param x: 形状batch_size,seq_len\n",
    "        :param state:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        # 目标句子词嵌入\n",
    "        x = self.embedding(x)\n",
    "        # 拿编码器ht状态中的某一个layer和输入进行拼接 进一步获取输入序列的信息\n",
    "        # layer的形状[batch_size,num_hidden]\n",
    "        # 将其重复seq_len次 变成[seq_len,batch_size,num_hidden]\n",
    "        context = decoder_state[-1].repeat(x.shape[1],1,1)\n",
    "        # 进行拼接 形状 [batch_size,seq_len,num_hidden+embedding_dim]\n",
    "        x_and_context = torch.cat((x, context.permute(1,0,2)), 2)\n",
    "        # 将拼接后的信息作为rnn输入 输入下一步的hidden和out\n",
    "        output, state = self.rnn(x_and_context, state)\n",
    "        output = self.classify(output)\n",
    "        return output, state"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Have a Try\n",
    "解码器,编码器的 num_hidden,num_layer需要一致"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "outputs": [
    {
     "data": {
      "text/plain": "(torch.Size([4, 7, 10]), torch.Size([6, 4, 256]))"
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.zeros((4,7),dtype=torch.long)\n",
    "encoder_output,decoder_state = encoder(x)\n",
    "decoder = Seq2SeqDecoder(vocab_size=10, embedding_dim=256, num_hidden=256,num_layers=6)\n",
    "decoder.eval()\n",
    "output,state = decoder(x,decoder_state)\n",
    "output.shape,state.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 实现一个用于屏蔽额外序列信息的长度\n",
    "为了批量加载数据,额外定义了PAD,SOS,EOS词\n",
    "对填充词元的预测应该排除在损失函数的计算之外"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "神奇的torch\n",
    "tensor的形状[M,N]\n",
    "对tensor进行 [:,None,:]的切片 会直接将tensor进行unsqueeze(1)的操作\n",
    "返回的tensor形状[M,1,N]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "outputs": [],
   "source": [
    "def sequence_mask(x:torch.Tensor,valid_len,value=0):\n",
    "    max_length = x.size(1)\n",
    "    mask = torch.arange(max_length, dtype=torch.float32, device=x.device)[None, :] < valid_len[:, None]\n",
    "    x[~mask] = value\n",
    "    return x"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "构造一个batch数据来测试"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[1, 2, 3],\n        [4, 5, 6]])"
     },
     "execution_count": 286,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor([[1, 2, 3], [4, 5, 6]])\n",
    "valid_len = torch.tensor([1,2])\n",
    "x"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[1, 0, 0],\n        [4, 5, 0]])"
     },
     "execution_count": 285,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masked_x = sequence_mask(x,valid_len)\n",
    "masked_x"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 拓展softmax遮蔽不相关的预测\n",
    "首先是构造一个全1矩阵weights\n",
    "用weights和valid_len调用sequence_mask生成屏蔽矩阵\n",
    "然后按照常规流程计算交叉熵损失\n",
    "最后用屏蔽矩阵屏蔽无效损失,对每一个词对应有效损失做均值,返回对应位置的损失"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "outputs": [],
   "source": [
    "class MaskedSoftmaxCELoss(nn.CrossEntropyLoss):\n",
    "    def forward(self, input: Tensor, target: Tensor,valid_len:Tensor,value=0) -> Tensor:\n",
    "        weights = torch.ones_like(target)\n",
    "        weights = sequence_mask(weights,valid_len,value)\n",
    "        self.reduction = 'none'\n",
    "        unweight_loss = super(MaskedSoftmaxCELoss, self).forward(input.permute(0,2,1),target)\n",
    "        weighted_loss = (unweight_loss*weights).mean(dim=1)\n",
    "        return weighted_loss"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "试一试\n",
    "input的形状[batch_size,seq_len,vocab_size]\n",
    "target的形状[batch_size,seq_len]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "outputs": [],
   "source": [
    "input,target = torch.ones(3, 4, 10),torch.ones((3, 4),dtype=torch.long)\n",
    "valid_len = torch.tensor([4,2,0])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "outputs": [],
   "source": [
    "loss = MaskedSoftmaxCELoss()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([2.3026, 1.1513, 0.0000])"
     },
     "execution_count": 314,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss(input,target,valid_len)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "先下班\n",
    "搞懂了一个点\n",
    "对于Decoder训练时可以将真实标签作为(教师强制)一次性输入\n",
    "但是Decoder在预测时,每次只能构造一个[1,1] (batch=1,长度为1的句子)输入\n",
    "获取其output,进行argmax求当前预测概率最大的单词\n",
    "然后用单词构造batch作为下一次的预测,直到到达设定长度,或者其预测结果为EOS停止"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}